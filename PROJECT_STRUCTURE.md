# SmartGlass AI Agent - Project Structure

## ðŸ“ Directory Structure

```
SmartGlass-AI-Agent/
â”œâ”€â”€ src/                          # Core source code (agent orchestration, backends, policies)
â”‚   â”œâ”€â”€ smartglass_agent.py      # Stable v1.0 SmartGlassAgent entry point
â”‚   â”œâ”€â”€ llm_backend_base.py      # Backend protocol for SNN/ANN/cloud text generators
â”‚   â”œâ”€â”€ llm_snn_backend.py       # On-device spiking student backend (SNN)
â”‚   â”œâ”€â”€ llm_backend.py           # ANN/text backends, routing, and tokenizer helpers
â”‚   â”œâ”€â”€ whisper_processor.py     # Whisper audio processing
â”‚   â”œâ”€â”€ clip_vision.py           # CLIP / DeepSeek-Vision processing
â”‚   â”œâ”€â”€ agent/                   # Policies, fusion, and action formatting
â”‚   â”œâ”€â”€ edge_runtime/            # Edge runtime toggles and privacy protections
â”‚   â””â”€â”€ skills/                  # Action execution shims and RaySkillKit bindings
â”‚   â”‚
â”‚   â”œâ”€â”€ world_model.py            # âœ… World state representation interface (Week 6)
â”‚   â”œâ”€â”€ clip_world_model.py       # âœ… CLIP-based world model implementation (Week 7)
â”‚   â”œâ”€â”€ context_store.py          # âœ… Memory store interface (Week 6)
â”‚   â”œâ”€â”€ sqlite_context_store.py   # âœ… SQLite context store with FTS5 (Week 7)
â”‚   â”œâ”€â”€ planner.py                # âœ… Task planning interface (Week 6)
â”‚   â”œâ”€â”€ rule_based_planner.py     # âœ… Rule-based planner implementation (Week 7)
â”‚   â”œâ”€â”€ telemetry.py              # âœ… Telemetry collection interface (Week 6)
â”‚   â””â”€â”€ safety/                  # Content moderation and guardrails
â”‚       â”œâ”€â”€ content_moderation.py  # RuleBasedModerator and SafetyGuard
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ drivers/                      # Device drivers and provider abstractions
â”‚   â”œâ”€â”€ providers/               # Provider resolver (mock, meta, vuzix, xreal, visionos, openxr)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ rayskillkit/                  # Skill/action execution adapters and payload schemas
â”œâ”€â”€ scripts/                      # Training, evaluation, and tooling scripts (e.g., SNN distillation)
â”œâ”€â”€ examples/                     # Usage examples and CLI demos
â”œâ”€â”€ tests/                        # âœ… Unit and integration tests (Week 7)
â”‚   â””â”€â”€ test_production_components.py  # Production component integration tests
â”œâ”€â”€ bench/                        # Performance benchmarking infrastructure
â”‚   â”œâ”€â”€ production_bench.py       # âœ… Production architecture benchmark (Week 7)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sdk-android/                  # Native Android client and bridge code
â”œâ”€â”€ sdk_python/                   # Python SDK distribution (pip-installable layout)
â”œâ”€â”€ colab_notebooks/              # Weekly notebooks and interactive workshops
â”œâ”€â”€ docs/                         # Documentation, reports, and integration guides
â”‚   â””â”€â”€ PRODUCTION_ARCHITECTURE.md  # âœ… Production component documentation (Week 7)
â”œâ”€â”€ validate_production_components.py  # âœ… Component validation script (Week 7)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.py                      # Package installation setup
â”œâ”€â”€ README.md                     # Main documentation
â”œâ”€â”€ QUICKSTART.md                 # Quick start guide
â”œâ”€â”€ CONTRIBUTING.md               # Contributing guidelines
â”œâ”€â”€ LICENSE                       # MIT License with NOTICE
â””â”€â”€ NOTICE.md                     # Third-party notices for the v1.0 stable release
```

## ðŸ”§ Core Modules

### 1. smartglass_agent.py
- **Purpose**: Main integration layer for multimodal queries
- **Features**:
  - Coordinates speech, vision, and language backends
  - Returns structured responses with `actions` aligned to RaySkillKit
  - Delegates provider selection to `drivers.providers.get_provider`
- **Use Case**: Stable entry point for apps and SDKs (Python/Android)

### 2. llm_backend_base.py & llm_snn_backend.py
- **Purpose**: Pluggable language generation backends
- **Backends**:
  - **SNNLLMBackend**: On-device spiking student for low-power glasses
  - **ANN/Cloud adapters**: GPT-style or hosted models via `llm_backend.py`
- **Features**:
  - Shared interface for `generate`/`chat` semantics
  - Tokenizer/model fallbacks to keep demos runnable without checkpoints
- **Use Case**: Swap between on-device SNN, local ANN, or remote providers without changing agent code

### 3. whisper_processor.py
- **Purpose**: Speech-to-text transcription
- **Model**: OpenAI Whisper (all sizes)
- **Features**:
  - Streaming and chunked audio support
  - Multilingual, with device-friendly configuration for edge runtime
- **Use Case**: Convert voice commands to text across providers

### 4. clip_vision.py
- **Purpose**: Visual understanding
- **Models**: OpenAI CLIP and DeepSeek-Vision
- **Features**:
  - Zero-shot image classification and captioning hooks
  - Scene understanding for action planning
- **Use Case**: Understand what the smart glasses see and feed context to the LLM backend

### 5. drivers.providers
- **Purpose**: Abstract device I/O (camera, mic, haptics) behind a provider interface
- **Providers**: mock, meta (Ray-Ban), vuzix, xreal, openxr, visionos
- **Use Case**: Target multiple devices while preserving a consistent API

### 6. rayskillkit and skills/
- **Purpose**: Map LLM-emitted `actions` to concrete skill implementations
- **Features**:
  - Action schemas for navigation, notifications, and device control
  - Adapters that bind RaySkillKit skills to provider capabilities
- **Use Case**: Execute actions on-device or via paired mobile/edge runtimes

### 7. world_model.py (Week 6 - Architecture Foundation)
- **Purpose**: World state representation and scene understanding
- **Interface**: WorldState, SceneObject, UserIntent dataclasses
- **Features**:
  - Update world state from vision processing
  - Query current state for planning context
- **Use Case**: Maintain persistent understanding of user environment across interactions

### 8. context_store.py (Week 6 - Architecture Foundation)
- **Purpose**: Memory persistence for experience frames
- **Interface**: ExperienceFrame, ContextQuery, ContextStore
- **Features**:
  - Write interaction history (query, response, actions, metadata)
  - Query past experiences for context retrieval
  - Session state summarization
- **Use Case**: Enable contextual awareness and conversation continuity

### 9. planner.py (Week 6 - Architecture Foundation)
- **Purpose**: Task decomposition and action planning
- **Interface**: Plan, PlanStep, Planner
- **Features**:
  - Decompose user intents into actionable steps
  - Generate execution plans with skill IDs and parameters
  - Constraint-based planning (safety mode, max steps)
- **Use Case**: Bridge high-level intents to executable actions with temporal ordering

### 10. telemetry.py (Week 6 - Architecture Foundation)
- **Purpose**: Structured event logging for observability
- **Components**: TelemetryEvent, TelemetryCollector, LatencyTracker
- **Event Types**: Latency, Error, Usage, Safety, Action
- **Features**:
  - Component-level latency tracking (ASR, Vision, LLM, E2E)
  - Error logging with severity levels
  - Safety moderation event tracking
  - In-memory and logging collectors for dev/test
- **Use Case**: Monitor performance, debug issues, track safety events in production

### 11. safety/ (Week 3-4 - Safety Implementation)
- **Purpose**: Content moderation and safety guardrails
- **Components**: RuleBasedModerator, SafetyGuard, ModerationResult
- **Features**:
  - Rule-based content filtering (violence, medical, dangerous activity, privacy)
  - Action filtering with severity-based blocking
  - Suggested fallback responses for unsafe content
- **Use Case**: GDPR/AI Act compliance, user safety, liability protection

## ðŸŽ¯ Production Architecture Status (Week 7-8)

### âœ… Completed Components

| Component | Status | Lines | Performance | Documentation |
|-----------|--------|-------|-------------|---------------|
| WorldModel Interface | âœ… | 125 | N/A | [src/world_model.py](src/world_model.py) |
| CLIPWorldModel | âœ… | 480 | 0.02ms P95 | [docs/PRODUCTION_ARCHITECTURE.md](docs/PRODUCTION_ARCHITECTURE.md) |
| ContextStore Interface | âœ… | 104 | N/A | [src/context_store.py](src/context_store.py) |
| SQLiteContextStore | âœ… | 419 | 15.09ms P95 (write) | [docs/PRODUCTION_ARCHITECTURE.md](docs/PRODUCTION_ARCHITECTURE.md) |
| Planner Interface | âœ… | 85 | N/A | [src/planner.py](src/planner.py) |
| RuleBasedPlanner | âœ… | 506 | 0.01ms P95 | [docs/PRODUCTION_ARCHITECTURE.md](docs/PRODUCTION_ARCHITECTURE.md) |
| TelemetryCollector | âœ… | 146 | N/A | [src/telemetry.py](src/telemetry.py) |
| **Total Production Code** | **7/7** | **1,865** | **15.60ms P95 E2E** | **100% documented** |

### ðŸ“Š Performance Benchmarks

- **E2E Workflow**: 9.20ms mean, 15.60ms P95 (âœ… 984ms under 1s target)
- **Intent Inference**: 0.01ms mean, 0.02ms P95  
- **Memory Write**: 8.85ms mean, 15.09ms P95
- **Memory Read**: 0.29ms mean, 0.36ms P95
- **Plan Generation**: 0.00ms mean, 0.01ms P95

Component breakdown: Memory (96.2%), Intent (0.1%), Planning (0.0%)

### ðŸ§ª Testing & Validation

- âœ… Integration tests: 4/4 pass (100%)
- âœ… Validation script: [validate_production_components.py](validate_production_components.py)
- âœ… Performance benchmark: [bench/production_bench.py](bench/production_bench.py)
- âœ… Component tests: [tests/test_production_components.py](tests/test_production_components.py)

### ðŸ“š Documentation

- âœ… Production Architecture Guide: [docs/PRODUCTION_ARCHITECTURE.md](docs/PRODUCTION_ARCHITECTURE.md)
- âœ… Usage examples with configuration best practices
- âœ… Troubleshooting and performance optimization tips

## ðŸ““ Notebooks

### SmartGlass_AI_Agent_Meta_RayBan.ipynb
**Target Audience**: Users and testers
**Content**:
- Setup and installation
- Component testing
- Multimodal integration
- Real-world scenarios for Meta Ray-Ban
- Upload and test with actual smart glass media

### SmartGlass_AI_Agent_Advanced.ipynb
**Target Audience**: Developers and researchers
**Content**:
- Custom configurations
- Performance optimization
- Real-time processing pipelines
- Custom use cases (Shopping, Accessibility)
- Benchmarking tools
- Deployment guidelines

## ðŸ“š Documentation

### README.md
- Project overview
- Quick start guide
- Features and capabilities
- Installation instructions
- Basic usage examples
- Meta Ray-Ban integration tips

### QUICKSTART.md
- 5-minute getting started guide
- Common use cases
- Configuration tips
- Troubleshooting

### CONTRIBUTING.md
- How to contribute
- Code style guidelines
- Development setup
- Testing guidelines
- Feature areas needing help

### docs/API_REFERENCE.md
- Complete API documentation
- All classes and methods
- Parameter descriptions
- Code examples
- Error handling

## ðŸŽ¯ Example Scripts

### examples/basic_usage.py
- Initialize the agent
- Text-only queries
- Conversation management
- Demonstrates core functionality

### examples/vision_example.py
- Vision processing
- Scene understanding
- Object classification
- Real-world use cases

## ðŸ§ª Test Coverage

### tests/test_safety_suite.py (Week 3-4)
- **Purpose**: Validate safety guardrails and content moderation
- **Coverage**: 32 test cases, 27 passing (84% pass rate)
- **Test Classes**:
  - TestContentModeration: Keyword-based filtering
  - TestActionModeration: Action safety validation
  - TestSafetyGuard: Integration with SmartGlassAgent
  - TestAdversarialCases: Edge cases and adversarial inputs
  - TestComplianceScenarios: GDPR/regulatory compliance
- **Known Gaps**: PII detection, ML-based moderation (documented for Week 10+ enhancement)

### tests/test_telemetry.py (Week 6)
- **Purpose**: Validate telemetry event collection and tracking
- **Coverage**: 18 test cases, 18 passing (100% pass rate)
- **Test Classes**:
  - TestTelemetryEvent: Event schema validation
  - TestInMemoryCollector: In-memory event storage for testing
  - TestLoggingCollector: Logging-based event collection
  - TestTelemetryCollectorHelpers: Convenience methods (latency, error, usage, safety)
  - TestLatencyTracker: Context manager for latency tracking
  - TestEndToEndTelemetry: E2E integration patterns
- **Features Tested**: Event serialization, filtering, error handling, multi-component latency

### tests/test_architecture_integration.py (Week 6)
- **Purpose**: Validate architecture component integration
- **Test Classes**:
  - TestArchitectureIntegration: Full stack integration with telemetry, world model, context store, planner
  - TestMockImplementations: Mock WorldModel, ContextStore, Planner for testing
- **Features Tested**: Telemetry collection during queries, world model updates, context store writes, planner integration, error handling
- **Mock Components**: Provides test doubles for architecture interfaces

## ðŸ”„ Data Flow

```
User Input (Audio/Text/Image)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SmartGlassAgent             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Audio Processing    â”‚     â”‚
â”‚   â”‚ (Whisper)           â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Vision Processing   â”‚     â”‚
â”‚   â”‚ (CLIP)              â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ Text Generation     â”‚     â”‚
â”‚   â”‚ (GPT-2)             â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Response to User
```

## ðŸš€ Typical Usage Flow

1. **Initialization**
   ```python
   agent = SmartGlassAgent()
   ```

2. **Capture Input**
   - Audio from Meta Ray-Ban microphone
   - Image from Meta Ray-Ban camera

3. **Process Query**
   ```python
   result = agent.process_multimodal_query(
       audio_input="command.wav",
       image_input="scene.jpg"
   )

    # Extract response with backward compatibility
    response_text = result.get("response", result) if isinstance(result, dict) else result
    actions = result.get("actions", []) if isinstance(result, dict) else []
    raw_payload = result.get("raw", {}) if isinstance(result, dict) else {}
   ```

4. **Get Response**
   - Text response from GPT-2 (see `response_text` above)
   - Optional structured actions/metadata available via `actions` / `raw`
   - (Optional) Convert the text response to speech for audio output

## ðŸ› ï¸ Configuration Options

### Model Selection
- **Whisper**: tiny, base, small, medium, large
- **CLIP**: vit-base-patch32, vit-large-patch14
- **GPT-2**: gpt2, gpt2-medium, gpt2-large, gpt2-xl

### Device Selection
- Auto-detect (recommended)
- CPU (slower, works everywhere)
- CUDA (faster, requires GPU)

### Performance Tuning
- Model size vs. accuracy tradeoff
- Batch processing
- Frame skipping for real-time
- Context window size

## ðŸ“Š File Sizes and Requirements

### Models (Disk Storage Requirements - Downloaded on First Run)
- Whisper tiny: ~39 MB
- Whisper base: ~74 MB
- Whisper small: ~244 MB
- CLIP base: ~350 MB
- GPT-2 base: ~500 MB

*Note: These are disk storage requirements for model files, not runtime memory usage.*

### Disk Space
- Minimum: 1 GB (tiny models)
- Recommended: 2-3 GB (base models)
- Full install: 5+ GB (large models)

### RAM Requirements (System Memory)
- Minimum: 4 GB (for tiny/base models, single image processing)
- Recommended: 8 GB (for base models with multimodal processing)
- Optimal: 16+ GB (for larger models or batch processing)

*Note: These are total system RAM requirements. GPU memory (VRAM) requirements are typically 2-4 GB for base models when using CUDA acceleration. Requirements increase with larger model sizes and concurrent processing.*

## ðŸ”Œ Extension Points

The system is designed to be extended:

1. **Add new models**: Replace or add models in respective processors
2. **Add new modalities**: Extend the agent with GPS, IMU, etc.
3. **Custom workflows**: Create custom use-case classes
4. **Platform integration**: Add mobile/embedded platform support

## ðŸ“ Version Information

- **Current Version**: 1.0.0
- **Status**: Stable SDK (SmartGlassAgent and core backends)
- **Python**: 3.9+
- **License**: MIT (see LICENSE and NOTICE for third-party attributions)

## ðŸ”— Key Dependencies

- torch >= 2.0.0
- transformers >= 4.30.0
- openai-whisper >= 20230314
- Pillow >= 9.5.0
- numpy >= 1.24.0

See `requirements.txt` for complete list.

---

**For detailed API documentation, see [API_REFERENCE.md](docs/API_REFERENCE.md)**
