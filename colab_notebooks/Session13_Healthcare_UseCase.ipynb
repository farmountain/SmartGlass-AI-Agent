{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farmountain/SmartGlass-AI-Agent/blob/main/colab_notebooks/Session13_Healthcare_UseCase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa1cd74",
      "metadata": {
        "id": "5fa1cd74"
      },
      "source": [
        "# üè• Week 13: Use Case ‚Äî Healthcare Smart Glass Assistant\n",
        "Simulate a Ray-Ban Meta smart glass assistant in a hospital or clinical setting. This notebook shows how to:\n",
        "\n",
        "- Capture and transcribe real-time doctor-patient interaction\n",
        "- Apply medical keyword extraction with spaCy\n",
        "- Auto-translate key terms\n",
        "- Suggest follow-up actions using GPT-2 or LLaMA2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97daeba5",
      "metadata": {
        "id": "97daeba5"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 0: RESTART RUNTIME first to clear all memory (important)\n",
        "# After restarting, run everything below\n",
        "\n",
        "# ‚úÖ Step 1: Install all dependencies (no SciSpacy)\n",
        "!pip install -q openai-whisper gtts pydub googletrans==4.0.0-rc1 spacy transformers\n",
        "\n",
        "# ‚úÖ Optional (for fallback noun-chunk extraction)\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "# ‚úÖ Step 3: Google Translate\n",
        "!pip install -U googletrans==4.0.0-rc1\n",
        "\n",
        "# ‚úÖ Step 4: Medical NLP\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bc5cdr_md-0.5.0.tar.gz\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio, display\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "\n",
        "# ‚úÖ 1. Simulate Audio Input (patient voice)\n",
        "tts = gTTS(\"Patient reports chest pain for two days and dizziness.\", lang='en')\n",
        "tts.save(\"session_audio.mp3\")\n",
        "sound = AudioSegment.from_file(\"session_audio.mp3\")\n",
        "sound.export(\"session_audio.wav\", format=\"wav\")\n",
        "\n",
        "# ‚úÖ 2. Whisper Transcription\n",
        "asr_model = whisper.load_model(\"base\")  # use \"tiny\" for CPU speed\n",
        "result = asr_model.transcribe(\"session_audio.wav\")\n",
        "print(\"ü©∫ Transcribed Text:\", result['text'])\n",
        "\n",
        "# ‚úÖ 3. Medical NER Extraction via BioBERT\n",
        "try:\n",
        "    print(\"üîç Loading BioBERT Medical NER model...\")\n",
        "    ner_pipeline = pipeline(\"ner\", model=\"kamalkraj/BioBERT-NER\", aggregation_strategy=\"simple\")\n",
        "    ner_results = ner_pipeline(result['text'])\n",
        "    keywords = [res['word'] for res in ner_results if res['score'] > 0.7]\n",
        "    print(\"üß† Extracted Medical Keywords:\", keywords)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è BioBERT model failed: {e}\")\n",
        "    keywords = []\n",
        "\n",
        "# ‚úÖ 4. Fallback: use spaCy noun chunks if no keywords found\n",
        "if not keywords:\n",
        "    print(\"üîÅ Falling back to noun-chunk extraction...\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(result['text'])\n",
        "    keywords = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) <= 3]\n",
        "    print(\"üß© Extracted noun chunks:\", keywords)\n",
        "\n",
        "# ‚úÖ 5. Translate medical keywords to Chinese\n",
        "translator = Translator()\n",
        "zh_keywords = []\n",
        "for word in keywords:\n",
        "    translated = translator.translate(word, dest='zh-CN')\n",
        "    zh_keywords.append(f\"{word} ‚Üí {translated.text}\")\n",
        "print(\"üåê Translated Keywords:\", zh_keywords)\n",
        "\n",
        "# ‚úÖ 6. Speak the Chinese translation\n",
        "chinese_summary = \"Ôºå\".join([t.split(\" ‚Üí \")[1] for t in zh_keywords])\n",
        "if chinese_summary.strip():\n",
        "    tts_zh = gTTS(chinese_summary, lang='zh-CN')\n",
        "    tts_zh.save(\"translated_keywords.mp3\")\n",
        "    display(Audio(\"translated_keywords.mp3\"))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No content to speak.\")\n",
        ""
      ],
      "metadata": {
        "id": "jX2-Kd6koTBx"
      },
      "id": "jX2-Kd6koTBx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}