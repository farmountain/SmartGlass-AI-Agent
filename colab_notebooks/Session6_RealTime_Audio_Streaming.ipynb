{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farmountain/SmartGlass-AI-Agent/blob/main/colab_notebooks/Session6_RealTime_Audio_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3TGlshOOsuS"
      },
      "source": [
        "# 🎙️ Session 06: Real-Time Audio Streaming\n",
        "\n",
        "In this session, we'll simulate real-time audio capture from a smart glass microphone, process it in short chunks, and transcribe it with Whisper.\n",
        "\n",
        "In production, this would run continuously on-device or stream via low-latency socket."
      ],
      "id": "f3TGlshOOsuS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0pO21rfOsuX",
        "outputId": "180be7bf-c224-417e-eff2-582877daa3da"
      },
      "source": [
        "# ✅ Install core libraries\n",
        "!pip install -q sounddevice scipy openai-whisper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m604.2/803.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "id": "A0pO21rfOsuX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0XHf8fLOsuZ",
        "outputId": "0a89234e-5ff0-4c50-97dc-5f9e2dcca5df"
      },
      "source": [
        "# ✅ Record short audio from microphone (10 seconds)\n",
        "# import sounddevice as sd # Cannot record directly from microphone in Colab\n",
        "from scipy.io.wavfile import write\n",
        "import whisper\n",
        "import numpy as np # Import numpy to generate dummy audio\n",
        "\n",
        "duration = 10  # seconds\n",
        "fs = 16000\n",
        "# print(\"🔴 Recording for 10 seconds...\") # Recording is not possible\n",
        "# audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
        "# sd.wait()\n",
        "\n",
        "# Generate dummy silent audio for demonstration\n",
        "audio = np.zeros(int(duration * fs), dtype='int16')\n",
        "\n",
        "write('realtime_input.wav', fs, audio)\n",
        "print(\"✅ Saved dummy silent audio as realtime_input.wav\") # Indicate dummy file is saved"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved dummy silent audio as realtime_input.wav\n"
          ]
        }
      ],
      "id": "x0XHf8fLOsuZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4CEgsqFOsua",
        "outputId": "5b46e4a1-6e8f-4cc6-ee92-7d540e39ee5e"
      },
      "source": [
        "# ✅ Transcribe audio using Whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"realtime_input.wav\")\n",
        "print(\"🗣️ Transcribed Text:\", result['text'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 134MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗣️ Transcribed Text: \n"
          ]
        }
      ],
      "id": "z4CEgsqFOsua"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScUnOxyXOsub"
      },
      "source": [
        "### 💡 Notes for Production\n",
        "\n",
        "- Use `pyaudio`, `ffmpeg`, or `webrtcvad` for true streaming.\n",
        "- Whisper's newer models support partial transcription.\n",
        "- Use socket streaming or direct hardware microphone access on real smart glasses (like Ray-Ban Meta SDK when it is available)."
      ],
      "id": "ScUnOxyXOsub"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}