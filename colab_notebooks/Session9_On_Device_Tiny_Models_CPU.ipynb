{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farmountain/SmartGlass-AI-Agent/blob/main/colab_notebooks/Session9_On_Device_Tiny_Models_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f493af7",
      "metadata": {
        "id": "8f493af7"
      },
      "source": [
        "# üì¶ Session 09: On-Device Tiny Models (CPU)\n",
        "Deploy compact models like DistilWhisper and MobileSAM for on-device use on smart glasses.\n",
        "\n",
        "This session covers:\n",
        "- Running CPU-optimized versions of Whisper and CLIP\n",
        "- Exporting to ONNX or TorchScript for efficient deployment\n",
        "- Benchmarks on inference time and memory usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9265bd06",
      "metadata": {
        "id": "9265bd06"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Install dependencies\n",
        "!pip install -q openai-whisper transformers torchaudio onnxruntime gTTS onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8263c3",
      "metadata": {
        "id": "af8263c3"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load DistilWhisper for speech recognition\n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "import whisper\n",
        "\n",
        "text = \"Hello, this is a test of gTTS.\"\n",
        "tts = gTTS(text=text, lang='en')\n",
        "audio_file = \"gtts_sample.mp3\"\n",
        "tts.save(audio_file)\n",
        "\n",
        "print(f\"Audio generated and saved as {audio_file}\")\n",
        "# Optional: Play the generated audio\n",
        "ipd.Audio(audio_file)\n",
        "\n",
        "model = whisper.load_model('tiny')\n",
        "\n",
        "# Transcribe the generated audio file\n",
        "result = model.transcribe(audio_file)\n",
        "print('üó£Ô∏è Transcription:', result['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584698c9",
      "metadata": {
        "id": "584698c9"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Convert Whisper model to ONNX for CPU inference\n",
        "import torch\n",
        "import whisper\n",
        "\n",
        "# Disable scaled dot product attention for ONNX export compatibility\n",
        "whisper.model.MultiHeadAttention.use_sdpa = False\n",
        "\n",
        "dummy_input = torch.randn(1, 80, 3000)\n",
        "# Add a dummy input for tokens\n",
        "dummy_tokens = torch.randint(0, model.decoder.token_embedding.num_embeddings, (1, 10))\n",
        "torch.onnx.export(model, (dummy_input, dummy_tokens), 'whisper_tiny.onnx', opset_version=11)\n",
        "print('‚úÖ ONNX model exported: whisper_tiny.onnx')\n",
        "\n",
        "# Re-enable scaled dot product attention\n",
        "whisper.model.MultiHeadAttention.use_sdpa = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf86cca7",
      "metadata": {
        "id": "cf86cca7"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load and benchmark ONNX model with onnxruntime\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "ort_session = ort.InferenceSession('whisper_tiny.onnx')\n",
        "\n",
        "# Get the input names from the ONNX session\n",
        "input_names = [inp.name for inp in ort_session.get_inputs()]\n",
        "\n",
        "# Prepare input feed dictionary using the correct input names\n",
        "input_feed = {input_names[0]: dummy_input.numpy(), input_names[1]: dummy_tokens.numpy()}\n",
        "\n",
        "start = time.time()\n",
        "outputs = ort_session.run(None, input_feed)\n",
        "end = time.time()\n",
        "print(f'‚è±Ô∏è ONNX Inference Time: {end - start:.3f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3dd29f",
      "metadata": {
        "id": "0e3dd29f"
      },
      "source": [
        "### üìå Notes:\n",
        "- Whisper's 'tiny' model runs efficiently on most CPUs\n",
        "- For production use, combine with ONNX Runtime or TorchScript\n",
        "- Consider using [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) or [tiny-CLIP](https://github.com/S-Lab-System-Group/TinyCLIP) for image tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b526101b",
      "metadata": {
        "id": "b526101b"
      },
      "source": [
        "üöÄ **Try optimizing for ARM-based edge devices (e.g., Raspberry Pi, Jetson Nano)** in future sessions.\n",
        "Use quantization-aware training or INT8 export for faster performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}