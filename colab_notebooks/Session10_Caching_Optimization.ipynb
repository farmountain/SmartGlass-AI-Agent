{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farmountain/SmartGlass-AI-Agent/blob/main/colab_notebooks/Session10_Caching_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ad6c8f",
      "metadata": {
        "id": "54ad6c8f"
      },
      "source": [
        "# üìò Session 10: Caching & Optimization\n",
        "**Goal:** Improve performance of audio/image/text pipelines using caching and inference optimization.\n",
        "\n",
        "We'll learn how to:\n",
        "- Cache model outputs using `joblib`, `pickle`, and `diskcache`\n",
        "- Use `TorchScript` or `ONNX` to optimize model inference\n",
        "- Apply audio/image/text pre-processing caching techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2fff96",
      "metadata": {
        "id": "3e2fff96"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Install dependencies\n",
        "!pip install -q torch torchvision torchaudio transformers openai-whisper onnxruntime diskcache joblib gtts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ecd7bb"
      },
      "source": [
        "# 2. Generate Audio\n",
        "from gtts import gTTS\n",
        "\n",
        "text_to_speak = \"Hello, this is an example audio file generated by gTTS.\"\n",
        "tts = gTTS(text=text_to_speak, lang='en')\n",
        "AUDIO_PATH = 'example.wav'\n",
        "tts.save(AUDIO_PATH)\n",
        "\n",
        "print(f'Generated audio file: {AUDIO_PATH}')"
      ],
      "id": "c2ecd7bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a76319e",
      "metadata": {
        "id": "7a76319e"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load Whisper and cache transcription results\n",
        "import whisper, os\n",
        "from gtts import gTTS\n",
        "from diskcache import Cache\n",
        "\n",
        "# Generate a sample audio if not exists\n",
        "AUDIO_PATH = 'example.wav'\n",
        "if not os.path.exists(AUDIO_PATH):\n",
        "    print(f\"Audio file '{AUDIO_PATH}' not found. Generating with gTTS...\")\n",
        "    tts = gTTS(text=\"Hello, this is an example audio file generated by gTTS.\", lang='en')\n",
        "    tts.save(AUDIO_PATH)\n",
        "    print(f'Generated audio file: {AUDIO_PATH}')\n",
        "\n",
        "# Load Whisper model and initialize cache\n",
        "model = whisper.load_model('tiny')\n",
        "cache = Cache('./whisper_cache')\n",
        "\n",
        "def cached_transcribe(audio_path):\n",
        "    if audio_path in cache:\n",
        "        print('üì¶ Using cached transcription')\n",
        "        return cache[audio_path]\n",
        "    result = model.transcribe(audio_path)\n",
        "    cache[audio_path] = result['text']\n",
        "    return result['text']\n",
        "\n",
        "text = cached_transcribe(AUDIO_PATH)\n",
        "print('üó£Ô∏è Text:', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9a82d7",
      "metadata": {
        "id": "dd9a82d7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "\n",
        "# Wrapper class to return only last_hidden_state\n",
        "class GPT2Wrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        output = self.model(input_ids)\n",
        "        return output.last_hidden_state\n",
        "\n",
        "# Wrap the model\n",
        "wrapped_model = GPT2Wrapper(model)\n",
        "\n",
        "# Sample input\n",
        "example = tokenizer(\"Hello from smart glasses!\", return_tensors='pt')\n",
        "\n",
        "# Trace the model\n",
        "traced_model = torch.jit.trace(wrapped_model, (example['input_ids'],))\n",
        "traced_model.save(\"gpt2_traced.pt\")\n",
        "\n",
        "print('üöÄ Traced GPT-2 model saved ‚úÖ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824dc500",
      "metadata": {
        "id": "824dc500"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Cache image features using CLIP\n",
        "from PIL import Image\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import torch\n",
        "import os\n",
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "clip_model.eval()\n",
        "\n",
        "# Allow user to upload an image file\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Assuming only one file is uploaded\n",
        "    image_filename = list(uploaded.keys())[0]\n",
        "    print(f\"Uploaded file: {image_filename}\")\n",
        "\n",
        "    try:\n",
        "        img = Image.open(image_filename)\n",
        "        input = clip_processor(images=img, return_tensors='pt')\n",
        "\n",
        "        # Use a filename based on the uploaded file for caching\n",
        "        cache_filename = f'{os.path.splitext(image_filename)[0]}_clip_features.pkl'\n",
        "\n",
        "        if os.path.exists(cache_filename):\n",
        "            print('üì¶ Loading cached features')\n",
        "            features = joblib.load(cache_filename)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                features = clip_model.get_image_features(**input)\n",
        "            joblib.dump(features, cache_filename)\n",
        "        print('üß† Features:', features.shape)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found after upload: {image_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}