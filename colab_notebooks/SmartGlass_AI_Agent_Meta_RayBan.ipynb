{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smartglass-header"
   },
   "source": [
    "# SmartGlass AI Agent - Meta Ray-Ban Testing Notebook\n",
    "\n",
    "This notebook provides a complete environment for testing the SmartGlass AI Agent with Meta Ray-Ban smart glasses.\n",
    "\n",
    "## Features\n",
    "- üé§ **Whisper**: Speech-to-text transcription\n",
    "- üëÅÔ∏è **CLIP**: Vision-language understanding\n",
    "- üí¨ **GPT-2**: Natural language generation\n",
    "\n",
    "## Compatible with:\n",
    "- Meta Ray-Ban Wayfarer\n",
    "- Meta Ray-Ban Stories\n",
    "- Any smart glasses with camera and audio capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers openai-whisper pillow numpy soundfile scipy opencv-python librosa\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "### Clone the Repository\n",
    "\n",
    "Clone the SmartGlass AI Agent repository to access the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-code"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/farmountain/SmartGlass-AI-Agent.git\n",
    "%cd SmartGlass-AI-Agent\n",
    "\n",
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "print(\"‚úÖ Repository cloned and path configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import-section"
   },
   "source": [
    "### Import Modules\n",
    "\n",
    "Import the SmartGlass AI Agent components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-modules"
   },
   "outputs": [],
   "source": [
    "from smartglass_agent import SmartGlassAgent\n",
    "from llm_snn_backend import SNNLLMBackend\n",
    "from whisper_processor import WhisperAudioProcessor\n",
    "from clip_vision import CLIPVisionProcessor\n",
    "from gpt2_generator import GPT2TextGenerator\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agent-init"
   },
   "source": [
    "## 2. Initialize SmartGlass AI Agent\n",
    "\n",
    "Initialize the agent with optimal settings for Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init-agent"
   },
   "outputs": [],
   "source": [
    "# Initialize the SmartGlass AI Agent\n",
    "agent = SmartGlassAgent(\n",
    "    whisper_model=\"base\",  # Good balance of speed and accuracy\n",
    "    clip_model=\"openai/clip-vit-base-patch32\",\n",
    "    gpt2_model=\"gpt2\",\n",
    "    device=None  # Auto-detect GPU if available\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: experiment with the SNN student backend placeholder\n",
    "snn_agent = SmartGlassAgent(\n",
    "    whisper_model=\"base\",\n",
    "    clip_model=\"openai/clip-vit-base-patch32\",\n",
    "    gpt2_model=\"gpt2\",\n",
    "    llm_backend=SNNLLMBackend(),\n",
    ")\n",
    "print(\"\\n‚úÖ SmartGlass AI Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agent-info"
   },
   "source": [
    "### Display Agent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-info"
   },
   "outputs": [],
   "source": [
    "# Display agent components information\n",
    "info = agent.get_agent_info()\n",
    "\n",
    "print(\"Agent Components:\")\n",
    "print(\"=\" * 60)\n",
    "for component, details in info.items():\n",
    "    print(f\"\\n{component.upper()}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-section"
   },
   "source": [
    "## 3. Test Individual Components\n",
    "\n",
    "Test each component separately before full integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-vision"
   },
   "source": [
    "### 3.1 Test Vision (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vision-test"
   },
   "outputs": [],
   "source": [
    "# Create a test image (you can upload your own)\n",
    "def create_test_image():\n",
    "    \"\"\"Create a sample test image.\"\"\"\n",
    "    img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "# Create or upload an image\n",
    "test_image = create_test_image()\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image')\n",
    "plt.show()\n",
    "\n",
    "# Test scene understanding\n",
    "print(\"\\nScene Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "scene_result = agent.analyze_scene(test_image)\n",
    "print(scene_result.get('description', 'No description available'))\n",
    "\n",
    "# Test object classification\n",
    "print(\"\\nObject Classification:\")\n",
    "print(\"=\" * 60)\n",
    "objects = ['person', 'car', 'building', 'nature', 'abstract pattern']\n",
    "identified = agent.identify_object(test_image, objects)\n",
    "print(f\"Identified as: {identified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-image"
   },
   "source": [
    "### Upload Your Own Image (from Meta Ray-Ban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-image-code"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Upload an image from your Meta Ray-Ban smart glasses\n",
    "print(\"Upload an image captured from your Meta Ray-Ban smart glasses:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process the first uploaded image\n",
    "if uploaded:\n",
    "    image_name = list(uploaded.keys())[0]\n",
    "    image_data = uploaded[image_name]\n",
    "    rayban_image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(rayban_image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Meta Ray-Ban Captured Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze the scene\n",
    "    print(\"\\nAnalyzing Meta Ray-Ban image...\")\n",
    "    print(\"=\" * 60)\n",
    "    scene_analysis = agent.analyze_scene(rayban_image)\n",
    "    print(scene_analysis.get('description', 'No description'))\n",
    "    \n",
    "    print(\"\\n‚úÖ Image analysis complete!\")\n",
    "else:\n",
    "    print(\"No image uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-audio"
   },
   "source": [
    "### 3.2 Test Audio (Whisper)\n",
    "\n",
    "Test speech-to-text transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "audio-test"
   },
   "outputs": [],
   "source": [
    "# Upload an audio file from Meta Ray-Ban\n",
    "print(\"Upload an audio file recorded from your Meta Ray-Ban smart glasses:\")\n",
    "uploaded_audio = files.upload()\n",
    "\n",
    "if uploaded_audio:\n",
    "    audio_name = list(uploaded_audio.keys())[0]\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    print(f\"\\nTranscribing audio: {audio_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    transcription = agent.process_audio_command(audio_name)\n",
    "    \n",
    "    print(f\"Transcribed text: {transcription}\")\n",
    "    print(\"\\n‚úÖ Audio transcription complete!\")\n",
    "else:\n",
    "    print(\"No audio file uploaded.\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"  text = agent.process_audio_command('audio_file.wav')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-text"
   },
   "source": [
    "### 3.3 Test Text Generation (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text-test"
   },
   "outputs": [],
   "source": [
    "# Test text generation\n",
    "print(\"Testing GPT-2 Text Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are smart glasses used for?\"\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "# Generate response\n",
    "response = agent.generate_response(test_query)\n",
    "print(f\"\\nResponse: {response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Text generation test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multimodal-section"
   },
   "source": [
    "## 4. Full Multimodal Integration\n",
    "\n",
    "Test the complete SmartGlass experience with audio, vision, and language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scenario-1"
   },
   "source": [
    "### Scenario 1: Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scenario-1-code"
   },
   "outputs": [],
   "source": [
    "# Scenario: User asks about what they're seeing\n",
    "print(\"Scenario: Visual Question Answering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use previously uploaded image or create a test one\n",
    "if 'rayban_image' in locals():\n",
    "    query_image = rayban_image\n",
    "else:\n",
    "    query_image = create_test_image()\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(query_image)\n",
    "plt.axis('off')\n",
    "plt.title('What the smart glasses see')\n",
    "plt.show()\n",
    "\n",
    "# User asks a question\n",
    "user_question = \"What do you see in this image?\"\n",
    "print(f\"\\nUser: {user_question}\")\n",
    "\n",
    "# Get response with visual context\n",
    "result = agent.help_identify(query_image, text_query=user_question)\n",
    "print(f\"\\nAgent: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scenario-2"
   },
   "source": [
    "### Scenario 2: Complete Multimodal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scenario-2-code"
   },
   "outputs": [],
   "source": [
    "# Scenario: Combine audio command with visual input\n",
    "print(\"Scenario: Multimodal Query (Text + Vision)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate a complete multimodal interaction\n",
    "text_command = \"Describe what I'm looking at and tell me if it's interesting\"\n",
    "\n",
    "# Process multimodal query\n",
    "result = agent.process_multimodal_query(\n",
    "    text_query=text_command,\n",
    "    image_input=query_image\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {result['query']}\")\n",
    "print(f\"\\nVisual Context: {result['visual_context']}\")\n",
    "print(f\"\\nResponse: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conversation-section"
   },
   "source": [
    "### Scenario 3: Interactive Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conversation-code"
   },
   "outputs": [],
   "source": [
    "# Scenario: Have a conversation with the agent\n",
    "print(\"Scenario: Interactive Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear previous conversation\n",
    "agent.clear_conversation_history()\n",
    "\n",
    "# Have a conversation\n",
    "conversation = [\n",
    "    \"Hello! Can you help me?\",\n",
    "    \"What time is it?\",\n",
    "    \"Thanks for your help!\"\n",
    "]\n",
    "\n",
    "print(\"\\nConversation:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for user_msg in conversation:\n",
    "    print(f\"\\nüë§ User: {user_msg}\")\n",
    "    response = agent.generate_response(user_msg)\n",
    "    print(f\"ü§ñ Agent: {response}\")\n",
    "\n",
    "# Show conversation history\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Conversation History:\")\n",
    "print(\"=\" * 60)\n",
    "history = agent.get_conversation_history()\n",
    "for entry in history:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom-section"
   },
   "source": [
    "## 5. Custom Use Cases for Meta Ray-Ban\n",
    "\n",
    "Explore specific use cases for Meta Ray-Ban smart glasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "use-case-1"
   },
   "source": [
    "### Use Case 1: Object Recognition Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "use-case-1-code"
   },
   "outputs": [],
   "source": [
    "# Object recognition for everyday items\n",
    "print(\"Use Case: Object Recognition Assistant\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define common objects\n",
    "common_objects = [\n",
    "    'phone', 'keys', 'wallet', 'book', 'cup', 'bottle',\n",
    "    'laptop', 'tablet', 'headphones', 'watch', 'glasses',\n",
    "    'pen', 'notebook', 'bag', 'card'\n",
    "]\n",
    "\n",
    "print(f\"\\nRecognizable objects: {', '.join(common_objects)}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  # Upload image from Meta Ray-Ban\")\n",
    "print(\"  object = agent.identify_object(image, common_objects)\")\n",
    "print(\"  print(f'I see your {object}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "use-case-2"
   },
   "source": [
    "### Use Case 2: Navigation Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "use-case-2-code"
   },
   "outputs": [],
   "source": [
    "# Navigation and scene understanding\n",
    "print(\"Use Case: Navigation Assistant\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "navigation_queries = [\n",
    "    \"indoor hallway\",\n",
    "    \"outdoor street\",\n",
    "    \"staircase\",\n",
    "    \"elevator\",\n",
    "    \"door or entrance\",\n",
    "    \"crosswalk\",\n",
    "    \"sidewalk\",\n",
    "    \"parking area\"\n",
    "]\n",
    "\n",
    "print(f\"\\nNavigation contexts: {', '.join(navigation_queries)}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  # Analyze scene for navigation\")\n",
    "print(\"  scene = agent.analyze_scene(image, custom_queries=navigation_queries)\")\n",
    "print(\"  print(f\\\"You are at: {scene['best_match']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "use-case-3"
   },
   "source": [
    "### Use Case 3: Text Reading Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "use-case-3-code"
   },
   "outputs": [],
   "source": [
    "# Text and document recognition\n",
    "print(\"Use Case: Text Reading Assistant\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "text_queries = [\n",
    "    \"document or paper\",\n",
    "    \"book or magazine\",\n",
    "    \"sign or label\",\n",
    "    \"menu\",\n",
    "    \"screen or display\",\n",
    "    \"handwritten note\"\n",
    "]\n",
    "\n",
    "print(f\"\\nText content types: {', '.join(text_queries)}\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  # Identify text content\")\n",
    "print(\"  content_type = agent.identify_object(image, text_queries)\")\n",
    "print(\"  print(f'I see a {content_type}')\")\n",
    "print(\"  # Then use OCR or other tools for actual text extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips-section"
   },
   "source": [
    "## 6. Tips for Meta Ray-Ban Integration\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Image Quality**: Ensure good lighting when capturing images with Ray-Ban\n",
    "2. **Audio Recording**: Record in quiet environments for better transcription\n",
    "3. **Processing Time**: Base models provide good balance of speed/accuracy\n",
    "4. **Battery Life**: Consider model size vs. battery consumption\n",
    "5. **Privacy**: Always respect privacy when using smart glasses in public\n",
    "\n",
    "### Performance Optimization:\n",
    "\n",
    "- Use `whisper_model=\"tiny\"` for faster transcription\n",
    "- Use `whisper_model=\"base\"` for better accuracy (recommended)\n",
    "- Enable GPU acceleration in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- Process images at lower resolution for faster results\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Deploy to edge device (Raspberry Pi, Jetson Nano)\n",
    "2. Create real-time processing pipeline\n",
    "3. Add text-to-speech for audio responses\n",
    "4. Implement custom training for specific use cases\n",
    "5. Integrate with Meta Ray-Ban API (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources-section"
   },
   "source": [
    "## 7. Resources and Documentation\n",
    "\n",
    "- **GitHub Repository**: https://github.com/farmountain/SmartGlass-AI-Agent\n",
    "- **Whisper Documentation**: https://github.com/openai/whisper\n",
    "- **CLIP Documentation**: https://github.com/openai/CLIP\n",
    "- **GPT-2 Documentation**: https://huggingface.co/gpt2\n",
    "- **Meta Ray-Ban**: https://www.ray-ban.com/usa/discover-ray-ban-stories\n",
    "\n",
    "### Support\n",
    "\n",
    "For issues and questions, please visit the GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ You're all set!\n",
    "\n",
    "Your SmartGlass AI Agent is ready to use with Meta Ray-Ban smart glasses.\n",
    "\n",
    "**Happy testing!** üëìü§ñ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SmartGlass_AI_Agent_Meta_RayBan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
