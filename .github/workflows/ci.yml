name: CI

on:
  push:
    branches:
      - main
      - 'release/**'
      - 'Week*'
  pull_request:

jobs:
  build:
    name: Build and Test
    runs-on: ubuntu-latest
    env:
      PROVIDER: mock

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install project dependencies
        run: |
          pip install -r requirements.txt
          pip install ruff pyright pytest
          if [ -f requirements-optional.txt ]; then
            pip install -r requirements-optional.txt
          fi
          if [ -f optional-requirements.txt ]; then
            pip install -r optional-requirements.txt
          fi

      - name: Validate skills schema
        id: validate_skills
        run: python cicd/validate_skills.py

      - name: Prepare artifacts directory
        run: mkdir -p artifacts

      - name: Generate sample manifest signature
        run: |
          python - <<'PY'
          import hashlib
          import json
          import os
          from pathlib import Path
          from nacl import encoding, signing

          seed = hashlib.sha256(b"SmartGlass sample signing key").digest()
          signing_key = signing.SigningKey(seed)
          os.environ["CI_MANIFEST_SIGNING_KEY"] = seed.hex()

          manifest = {
              "models": [
                  {"path": "example.bin", "sha256": "00" * 32, "size": 0},
              ],
              "stats": [
                  {"path": "metrics.json", "sha256": "11" * 32, "size": 128},
              ],
          }

          manifest_path = Path("artifacts/sample_release_manifest.json")
          manifest_path.write_text(json.dumps(manifest, indent=2, sort_keys=True) + "\n", encoding="utf-8")

          verify_path = Path("artifacts/sample_release_manifest.pub")
          verify_path.write_text(
              signing_key.verify_key.encode(encoder=encoding.HexEncoder).decode("ascii") + "\n",
              encoding="utf-8",
          )
          PY

          python cicd/sign_manifest.py \
            --in artifacts/sample_release_manifest.json \
            --out artifacts/sample_release_manifest.sig \
            --key-env CI_MANIFEST_SIGNING_KEY

      - name: Run redteam health smoke test
        id: redteam_health
        run: |
          python redteam/eval.py --scenarios redteam/health.yaml --out artifacts/redteam_health_report.json
          python - <<'PY'
          import json
          from pathlib import Path

          report_path = Path("artifacts/redteam_health_report.json")
          data = json.loads(report_path.read_text(encoding="utf-8"))
          summary = data.get("summary", {})
          if not summary.get("all_passed", False):
              raise SystemExit("Redteam health smoke test failed: scenarios did not all pass")
          PY

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '17'

      - name: Cache Gradle packages
        id: gradle_cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.gradle/caches
            ~/.gradle/wrapper
          key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}
          restore-keys: |
            ${{ runner.os }}-gradle-

      - name: Assemble Android SDK
        id: android_assemble
        run: ./gradlew :sdk-android:assemble

      - name: Upload Android AAR artifact
        id: upload_android_aar
        if: ${{ success() && steps.android_assemble.outcome == 'success' }}
        uses: actions/upload-artifact@v4
        with:
          name: android-aar
          path: sdk-android/build/outputs/aar/*.aar
          if-no-files-found: error

      - name: Run ruff
        id: ruff
        run: ruff check .

      - name: Run pyright
        id: pyright
        continue-on-error: ${{ contains(github.ref_name, 'Week1') || contains(github.ref_name, 'week-1') || contains(github.ref_name, 'week1') || contains(github.head_ref, 'Week1') || contains(github.head_ref, 'week-1') || contains(github.head_ref, 'week1') }}
        run: pyright

      - name: Run pytest
        id: pytest
        run: pytest

      - name: Run image benchmarks
        id: image_bench
        if: ${{ hashFiles('bench/image_bench.py') != '' }}
        run: |
          mkdir -p artifacts
          python bench/image_bench.py || true

      - name: Run audio benchmarks
        id: audio_bench
        if: ${{ hashFiles('bench/audio_bench.py') != '' }}
        run: |
          mkdir -p artifacts
          python bench/audio_bench.py || true

      - name: Run hero benchmark
        id: hero_bench
        if: ${{ hashFiles('bench/e2e_demo1_bench.py') != '' }}
        run: |
          mkdir -p artifacts
          python bench/e2e_demo1_bench.py || true

      - name: Inventory repository
        id: inventory
        run: python scripts/inventory_repo.py --strict

      - name: Run latency benchmark
        id: latency
        if: ${{ hashFiles('bench/latency_bench.py') != '' }}
        run: |
          set -o pipefail
          if python bench/latency_bench.py --help >/tmp/latency_help.txt 2>&1; then
            if grep -q -- '--output' /tmp/latency_help.txt; then
              python bench/latency_bench.py --output artifacts/latency.csv
            else
              python bench/latency_bench.py | tee artifacts/latency.csv
            fi
          else
            python bench/latency_bench.py | tee artifacts/latency.csv
          fi

      - name: Run red-team evaluations
        id: redteam
        if: ${{ hashFiles('redteam/eval.py') != '' }}
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob
          mkdir -p artifacts

          scenario_files=(redteam/*.yaml)
          if [ ${#scenario_files[@]} -eq 0 ]; then
            echo "No red-team scenario files found; skipping."
            exit 0
          fi

          report_paths=()
          for scenario in "${scenario_files[@]}"; do
            base_name=$(basename "$scenario" .yaml)
            out_path="artifacts/redteam_${base_name}_report.json"
            echo "::group::Evaluating ${scenario}"
            python redteam/eval.py --scenarios "$scenario" --out "$out_path"
            echo "Report written to $out_path"
            echo "::endgroup::"
            report_paths+=("$out_path")
          done

          python - <<'PY' "${report_paths[@]}"
import json
import sys
from pathlib import Path

report_paths = sys.argv[1:]
if not report_paths:
    raise SystemExit("No report paths were provided to the aggregator.")

overall_total = 0
overall_passed = 0
deny_total = 0
deny_failures = []
lines = []

for report_path in report_paths:
    path = Path(report_path)
    data = json.loads(path.read_text(encoding="utf-8"))
    summary = data.get("summary", {})
    total = int(summary.get("total", 0))
    passed = int(summary.get("passed", 0))
    overall_total += total
    overall_passed += passed

    scenarios_file = data.get("scenarios_file", path.name)
    pass_rate = summary.get("pass_rate")
    if pass_rate is None and total:
        pass_rate = passed / total

    if total:
        if pass_rate is not None:
            percent = pass_rate * 100
            lines.append(f"- {scenarios_file}: {passed}/{total} passed ({percent:.1f}% pass rate)")
        else:
            lines.append(f"- {scenarios_file}: {passed}/{total} passed")
    else:
        lines.append(f"- {scenarios_file}: no scenarios found")

    for result in data.get("results", []):
        if result.get("expected_decision") == "deny":
            deny_total += 1
            if result.get("decision") != "deny":
                scenario_id = result.get("id") or result.get("scenario_id") or "unknown"
                deny_failures.append(f"{scenario_id} ({scenarios_file})")

if overall_total == 0:
    raise SystemExit("Red-team evaluation ran, but no scenarios were executed.")

overall_rate = (overall_passed / overall_total) * 100
print("Red-team evaluation summary:")
for line in lines:
    print(line)
print(f"Overall: {overall_passed}/{overall_total} scenarios passed ({overall_rate:.1f}% pass rate)")

if deny_total:
    deny_passed = deny_total - len(deny_failures)
    print(f"Deny scenarios: {deny_passed}/{deny_total} correctly returned 'deny'")

if deny_failures:
    print("The following deny scenarios were not denied:")
    for entry in deny_failures:
        print(f"  - {entry}")
    raise SystemExit("One or more deny scenarios failed to return 'deny'.")

if overall_passed != overall_total:
    raise SystemExit("Not all red-team scenarios passed.")
PY

      - name: Summarize results
        if: ${{ always() }}
        env:
          GITHUB_REF_NAME: ${{ github.ref_name }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}
          PROVIDER: ${{ env.PROVIDER }}
        run: |
          reversal_flag_file="artifacts/reversal_gate.flag"
          rm -f "$reversal_flag_file"
          {
            echo "## CI Summary";
            echo "";
            echo "| Step | Status |";
            echo "| --- | --- |";
            if [ "${{ steps.validate_skills.outcome }}" != "" ]; then
              echo "| Validate Skills | ${{ steps.validate_skills.outcome }} |";
            fi
            if [ "${{ steps.gradle_cache.outcome }}" != "" ]; then
              echo "| Gradle Cache | ${{ steps.gradle_cache.outcome }} |";
            fi
            if [ "${{ steps.android_assemble.outcome }}" != "" ]; then
              echo "| Android Assemble | ${{ steps.android_assemble.outcome }} |";
            fi
            if [ "${{ steps.upload_android_aar.outcome }}" != "" ]; then
              echo "| Upload Android AAR | ${{ steps.upload_android_aar.outcome }} |";
            fi
            echo "| Ruff | ${{ steps.ruff.outcome }} |";
            echo "| Pyright | ${{ steps.pyright.outcome }} |";
            echo "| Pytest | ${{ steps.pytest.outcome }} |";
            echo "| Inventory | ${{ steps.inventory.outcome }} |";

            if [ "${{ steps.latency.outcome }}" != "" ]; then
              echo "| Latency Bench | ${{ steps.latency.outcome }} |"; 
            fi
            if [ "${{ steps.redteam.outcome }}" != "" ]; then
              echo "| Red Team Eval | ${{ steps.redteam.outcome }} |";
            fi
            if [ "${{ steps.audio_bench.outcome }}" != "" ]; then
              echo "| Audio Bench | ${{ steps.audio_bench.outcome }} |";
            fi
            if [ "${{ steps.image_bench.outcome }}" != "" ]; then
              echo "| Image Bench | ${{ steps.image_bench.outcome }} |";
            fi
            if [ "${{ steps.hero_bench.outcome }}" != "" ]; then
              echo "| Hero Bench | ${{ steps.hero_bench.outcome }} |";
            fi
            echo "";
            echo "Provider: ${PROVIDER:-unknown}";
            echo "";
            echo "### Wearables SDK";
            echo "";
            echo "- Provider: ${PROVIDER:-unknown}";
            if [ -f artifacts/metrics.jsonl ]; then
              python - <<'PY'
import json
from pathlib import Path

path = Path("artifacts/metrics.jsonl")
capabilities_entry = None
permissions_entry = None

if path.exists():
    with path.open() as stream:
        for line in stream:
            line = line.strip()
            if not line:
                continue
            try:
                entry = json.loads(line)
            except json.JSONDecodeError:
                continue

            metric = entry.get("metric")
            if metric == "sdk.capabilities" and capabilities_entry is None:
                capabilities_entry = entry
            if metric == "permissions.time_to_ready_ms" and permissions_entry is None:
                permissions_entry = entry

            if capabilities_entry is not None and permissions_entry is not None:
                break


def _format_value(entry):
    if not entry:
        return None
    for key in ("value", "values", "capabilities", "metrics"):
        if key in entry and entry[key] not in (None, ""):
            value = entry[key]
            if isinstance(value, dict):
                if "value" in value and value["value"] not in (None, ""):
                    inner = value["value"]
                    if isinstance(inner, (dict, list, tuple)):
                        return json.dumps(inner, ensure_ascii=False)
                    return inner
                if "time_to_ready_ms" in value and value["time_to_ready_ms"] not in (None, ""):
                    return value["time_to_ready_ms"]
                return json.dumps(value, ensure_ascii=False)
            if isinstance(value, (list, tuple)):
                return json.dumps(value, ensure_ascii=False)
            return value
    return json.dumps(entry, ensure_ascii=False)


capabilities_value = _format_value(capabilities_entry)
permissions_value = _format_value(permissions_entry)

if capabilities_value is not None:
    print(f"- Capabilities: {capabilities_value}")
if permissions_value is not None:
    print(f"- Permissions time to ready (ms): {permissions_value}")
PY
            else
              echo "- Metrics: _artifacts/metrics.jsonl not found_.";
            fi
            echo "";
            if [ -f artifacts/audio_latency.csv ]; then
              echo "### Audio Latency Metrics";
              echo "";
              python - <<'PY'
import csv
from pathlib import Path

path = Path("artifacts/audio_latency.csv")
rows = []
with path.open(newline="") as f:
    reader = csv.reader(f)
    rows = [row for row in reader if any(cell.strip() for cell in row)]

if not rows:
    print("_Audio latency metrics file was empty (soft gate active)._\n")
else:
    header, *body = rows
    print("| " + " | ".join(header) + " |")
    print("| " + " | ".join("---" for _ in header) + " |")
    for row in body:
        print("| " + " | ".join(row) + " |")
    print()
PY
            else
              echo "_Audio latency metrics unavailable (Week 2 soft gate)._";
              echo "";
            fi
            if [ -f artifacts/image_latency.csv ]; then
              echo "### Vision Latency Metrics";
              echo "";
              python - <<'PY'
import csv
from pathlib import Path


def render_csv(path: Path, empty_message: str) -> None:
    with path.open(newline="") as f:
        reader = csv.reader(f)
        rows = [row for row in reader if any(cell.strip() for cell in row)]

    if not rows:
        print(empty_message)
        print()
        return

    header, *body = rows
    print("| " + " | ".join(header) + " |")
    print("| " + " | ".join("---" for _ in header) + " |")
    for row in body:
        print("| " + " | ".join(row) + " |")
    print()


render_csv(Path("artifacts/image_latency.csv"), "_Image latency metrics file was empty._")
PY
            fi
            if [ -f artifacts/e2e_hero1_summary.json ]; then
              echo "### Hero Caption Benchmark";
              echo "";
              python - <<'PY'
import json
from pathlib import Path

summary_path = Path("artifacts/e2e_hero1_summary.json")

summary = json.loads(summary_path.read_text())
stage_stats = summary.get("stages", {})
runs = summary.get("runs", 0)
totals = summary.get("totals", {})
fusion = summary.get("fusion", {})
aggregates = summary.get("aggregates", {})
providers = aggregates.get("providers") or summary.get("providers") or []

print(f"_Runs: {runs}_")
print()
if stage_stats:
    print("| Stage | p50 (ms) | p95 (ms) |")
    print("| --- | --- | --- |")
    for stage, stats in stage_stats.items():
        label = stage.replace("_", " ")
        print(f"| {label} | {stats.get('p50_ms', 0.0):.3f} | {stats.get('p95_ms', 0.0):.3f} |")
    if totals:
        print(f"| total | {totals.get('p50_ms', 0.0):.3f} | {totals.get('p95_ms', 0.0):.3f} |")
    print()
else:
    print("_No hero latency metrics available._")
    print()

caption_p50 = aggregates.get("caption_p50_ms")
vision_audio_p50 = aggregates.get("vision_audio_p50_ms")
total_p50 = aggregates.get("total_p50_ms") or totals.get("p50_ms")
total_p95 = aggregates.get("total_p95_ms") or totals.get("p95_ms")

if caption_p50 is not None or vision_audio_p50 is not None:
    print("#### End-to-End Metrics")
    print()
    header = ["Provider", "total p50 (ms)", "total p95 (ms)", "caption p50 (ms)", "vision+audio p50 (ms)"]
    print("| " + " | ".join(header) + " |")
    print("| " + " | ".join("---" for _ in header) + " |")
    provider_label = ", ".join(providers) if providers else "unknown"
    print(
        "| {provider} | {total_p50:.3f} | {total_p95:.3f} | {caption:.3f} | {vision_audio:.3f} |".format(
            provider=provider_label,
            total_p50=float(total_p50 or 0.0),
            total_p95=float(total_p95 or 0.0),
            caption=float(caption_p50 or 0.0),
            vision_audio=float(vision_audio_p50 or 0.0),
        )
    )
    print()

if fusion:
    print("#### Fusion Metrics")
    print()
    print("| Metric | Value |")
    print("| --- | --- |")
    print(f"| audio p50 (ms) | {fusion.get('audio_p50_ms', 0.0):.3f} |")
    print(f"| vision p50 (ms) | {fusion.get('vision_p50_ms', 0.0):.3f} |")
    print(f"| combined p50 (ms) | {fusion.get('combined_p50_ms', 0.0):.3f} |")
    print(f"| mean score | {fusion.get('score_mean', 0.0):.3f} |")
    print(f"| mean audio confidence | {fusion.get('audio_conf_mean', 0.0):.3f} |")
    print(f"| mean vision confidence | {fusion.get('vision_conf_mean', 0.0):.3f} |")
    print()

fsm_counts = summary.get("fsm", {}).get("final_state_counts", {})
if fsm_counts:
    print("#### FSM Final State Distribution")
    print()
    print("| State | Count |")
    print("| --- | --- |")
    for state, count in fsm_counts.items():
        print(f"| {state} | {count} |")
    print()

threshold_warnings = []
if caption_p50 is not None and float(caption_p50) > 600.0:
    threshold_warnings.append(
        ":warning: caption p50 {:.1f} ms exceeds 600 ms target.".format(float(caption_p50))
    )
if vision_audio_p50 is not None and float(vision_audio_p50) > 900.0:
    threshold_warnings.append(
        ":warning: vision+audio p50 {:.1f} ms exceeds 900 ms target.".format(float(vision_audio_p50))
    )

if threshold_warnings:
    print("#### Hero Warnings")
    print()
    for line in threshold_warnings:
        print(f"- {line}")
    print()

if threshold_warnings:
    warn_path = Path("artifacts/e2e_hero1_warnings.json")
    warn_path.write_text(json.dumps(threshold_warnings, indent=2))
PY
            fi
            if [ -f artifacts/ocr_results.csv ]; then
              echo "### Synthetic OCR Metrics";
              echo "";
              python - <<'PY'
import csv
from pathlib import Path


def render_csv(path: Path, empty_message: str) -> None:
    with path.open(newline="") as f:
        reader = csv.reader(f)
        rows = [row for row in reader if any(cell.strip() for cell in row)]

    if not rows:
        print(empty_message)
        print()
        return

    header, *body = rows
    print("| " + " | ".join(header) + " |")
    print("| " + " | ".join("---" for _ in header) + " |")
    for row in body:
        print("| " + " | ".join(row) + " |")
    print()


render_csv(Path("artifacts/ocr_results.csv"), "_Synthetic OCR metrics file was empty._")
PY
            fi
            if [ -f artifacts/metrics.jsonl ]; then
              REVERSAL_FLAG_PATH="$reversal_flag_file" python - <<'PY'
import json
import os
import re
import sys
from collections import defaultdict, deque
from pathlib import Path

path = Path("artifacts/metrics.jsonl")
DEFAULT_PROVIDER = os.getenv("PROVIDER", "unknown") or "unknown"
vision_rates = {}
ocr_precision = None
tracked_specs = {
    "asr.total_partials": "ASR total partials",
    "asr.reversal_rate": "ASR reversal rate",
    "audio_bench.reversal_rate": "Audio bench reversal rate",
    "asr.first_partial_ms": "ASR first partial (ms)",
    "vad.frames_processed": "VAD frames processed",
}
reversal_sources = (
    "audio_bench.reversal_rate",
    "asr.reversal_rate",
)
provider_metrics: dict[str, dict[str, dict[str, object | None]]] = {}
latest = defaultdict(lambda: {"asr": None, "vad": None})


def _make_record_table() -> dict[str, dict[str, object | None]]:
    return {
        metric: {"label": label, "value": None, "timestamp": None}
        for metric, label in tracked_specs.items()
    }


def _coerce_float(value):
    try:
        return float(value)
    except Exception:
        return None


def _stringify(value):
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, str):
        return value
    try:
        return json.dumps(value, ensure_ascii=False)
    except TypeError:
        return str(value)


def classify(entry):
    labels = []
    for key in ("task", "metric", "name", "tag", "category", "label"):
        value = entry.get(key)
        if isinstance(value, str):
            labels.append(value.lower())
    blob = " ".join(labels)
    if "asr" in blob or "speech_recognition" in blob:
        return "asr"
    if "vad" in blob or "voice_activity" in blob:
        return "vad"
    return None


def extract_value(entry):
    for key in ("value", "score", "wer", "f1", "accuracy", "metric_value"):
        if key in entry and isinstance(entry[key], (int, float, str)):
            return entry[key]
    metrics = entry.get("metrics")
    if isinstance(metrics, dict):
        for key in ("value", "score", "wer", "f1", "accuracy"):
            if key in metrics:
                return metrics[key]
    return json.dumps(entry, ensure_ascii=False)


def _init_fusion_fsm():
    return {
        "fusion.alpha_last": None,
        "fusion.alpha_avg": None,
        "tts.ms": None,
        "overlay.render_ms": None,
        "transitions": [],
    }


FUSION_METRIC_LABELS = {
    "fusion.alpha_last": "Fusion α last",
    "fusion.alpha_avg": "Fusion α avg",
    "tts.ms": "TTS (ms)",
    "overlay.render_ms": "Overlay render (ms)",
}


def _format_metric_value(entry):
    if not entry:
        return "n/a"
    value = extract_value(entry)
    if value is None:
        return "n/a"
    numeric = _coerce_float(value)
    if numeric is not None:
        if float(numeric).is_integer():
            return str(int(numeric))
        return f"{numeric:.3f}"
    return _stringify(value)


def _format_metric_details(entry):
    if not entry:
        return "n/a"
    parts = []
    timestamp = entry.get("timestamp") or entry.get("step")
    if timestamp not in (None, ""):
        parts.append(f"ts={timestamp}")
    tags = dict((entry.get("tags") or {}))
    tags.pop("provider", None)
    if tags:
        parts.append("tags=" + json.dumps(tags, ensure_ascii=False))
    details = entry.get("details")
    if details not in (None, ""):
        parts.append("details=" + _stringify(details))
    return "; ".join(parts) if parts else "n/a"


def _summarize_transitions(entries):
    if not entries:
        return "n/a", "n/a"
    counts = defaultdict(int)
    last_entry = None
    for entry in entries:
        tags = entry.get("tags") or {}
        from_state = tags.get("from") or tags.get("source") or tags.get("prev") or tags.get("state") or "?"
        to_state = (
            tags.get("to")
            or tags.get("dest")
            or tags.get("next")
            or tags.get("target")
            or tags.get("state_to")
            or "?"
        )
        label = f"{from_state}->{to_state}"
        counts[label] += 1
        last_entry = entry
    summary = ", ".join(f"{label} ({count})" for label, count in sorted(counts.items()))
    details = _format_metric_details(last_entry)
    return summary or "n/a", details


entries_tail = deque(maxlen=200)
fusion_fsm_data = defaultdict(_init_fusion_fsm)
with path.open() as stream:
    for line in stream:
        line = line.strip()
        if not line:
            continue
        try:
            entry = json.loads(line)
        except json.JSONDecodeError:
            continue
        entries_tail.append(entry)
        tags = entry.get("tags") or {}
        provider = str(tags.get("provider") or DEFAULT_PROVIDER)
        bucket = classify(entry)
        if bucket and entry:
            latest[provider][bucket] = entry
        metric = entry.get("metric")
        if metric in tracked_specs:
            records = provider_metrics.setdefault(provider, _make_record_table())
            record = records[metric]
            value = entry.get("value")
            if value is None:
                for key in ("score", "wer", "f1", "accuracy", "metric_value"):
                    if key in entry:
                        value = entry[key]
                        break
            if value is None:
                metrics_blob = entry.get("metrics")
                if isinstance(metrics_blob, dict):
                    for key in ("value", "score", "wer", "f1", "accuracy"):
                        if key in metrics_blob:
                            value = metrics_blob[key]
                            break
            record["value"] = value
            timestamp = entry.get("timestamp")
            if timestamp is None:
                timestamp = entry.get("step")
            if timestamp is None:
                details = entry.get("details")
                if isinstance(details, (str, int, float)):
                    timestamp = details
            record["timestamp"] = _stringify(timestamp)
        if metric == "vision.keys_rate":
            clip = str(tags.get("clip", "unknown"))
            vision_rates[clip] = entry
        elif metric == "ocr.precision_synth":
            ocr_precision = entry


for entry in entries_tail:
    metric = entry.get("metric")
    if metric not in FUSION_METRIC_LABELS and metric != "fsm.transitions":
        continue
    provider = str((entry.get("tags") or {}).get("provider") or DEFAULT_PROVIDER)
    data = fusion_fsm_data[provider]
    if metric == "fsm.transitions":
        data["transitions"].append(entry)
    else:
        data[metric] = entry


def _is_week4_or_later(ref):
    if not ref:
        return False
    match = re.search(r"week[-_\s]?(\d+)", ref, re.IGNORECASE)
    if match:
        try:
            return int(match.group(1)) >= 4
        except ValueError:
            return False
    return False


ref_candidates = [
    os.getenv("GITHUB_REF_NAME", ""),
    os.getenv("GITHUB_HEAD_REF", ""),
]
week_gate_active = any(_is_week4_or_later(ref) for ref in ref_candidates)
reversal_details = []
records_for_gate = provider_metrics.get(DEFAULT_PROVIDER)
if records_for_gate is None and provider_metrics:
    records_for_gate = next(iter(provider_metrics.values()))
if records_for_gate:
    for source in reversal_sources:
        record = records_for_gate.get(source)
        if not record:
            continue
        value = _coerce_float(record.get("value"))
        if value is not None:
            reversal_details.append((source, value))

flag_path = Path(os.getenv("REVERSAL_FLAG_PATH", "artifacts/reversal_gate.flag"))
reversal_warning_message = None
if week_gate_active and reversal_details:
    preferred_source, preferred_value = reversal_details[0]
    if preferred_value > 0.5:
        flag_path.parent.mkdir(parents=True, exist_ok=True)
        flag_path.write_text(f"{preferred_value:.6f}")
        reversal_warning_message = (
            f":warning: Audio reversal rate {preferred_value:.3f} exceeded the 0.5 threshold for Week 4+ runs."
        )
elif flag_path.exists():
    flag_path.unlink()


print("### ASR/VAD Metrics")
print()

if not provider_metrics:
    print("_ASR/VAD metrics not yet available (soft gate)._\n")
else:
    has_tracked_values = any(
        record["value"] is not None
        for metrics in provider_metrics.values()
        for record in metrics.values()
    )
    printed_latest = False
    for provider, metrics in sorted(provider_metrics.items()):
        print(f"#### Provider: {provider}")
        print()
        print("| Metric | Value | Timestamp |")
        print("| --- | --- | --- |")
        for metric in (
            "asr.total_partials",
            "asr.reversal_rate",
            "audio_bench.reversal_rate",
            "asr.first_partial_ms",
            "vad.frames_processed",
        ):
            record = metrics[metric]
            value = record["value"]
            timestamp = record["timestamp"]
            value_display = "n/a" if value is None else _stringify(value)
            timestamp_display = timestamp if timestamp is not None else "n/a"
            print(f"| {record['label']} | {value_display} | {timestamp_display} |")
        print()

        fusion_block = fusion_fsm_data.get(provider)
        has_fusion_metrics = False
        if fusion_block:
            has_named_metrics = any(
                fusion_block.get(key) is not None
                for key in FUSION_METRIC_LABELS
            )
            has_transitions = bool(fusion_block.get("transitions"))
            has_fusion_metrics = has_named_metrics or has_transitions
        if has_fusion_metrics:
            print("##### Fusion/FSM")
            print()
            print("| Metric | Value | Details |")
            print("| --- | --- | --- |")
            for metric_key, label in FUSION_METRIC_LABELS.items():
                entry = fusion_block.get(metric_key) if fusion_block else None
                value_display = _format_metric_value(entry)
                detail_display = _format_metric_details(entry)
                print(f"| {label} | {value_display} | {detail_display} |")
            summary, detail = _summarize_transitions(fusion_block.get("transitions") if fusion_block else None)
            print(f"| FSM transitions | {summary} | {detail} |")
            print()

        provider_latest = latest.get(provider, {"asr": None, "vad": None})
        if any(provider_latest.values()):
            printed_latest = True
            print("| Metric | Value | Notes |")
            print("| --- | --- | --- |")
            for key, label in (("asr", "ASR"), ("vad", "VAD")):
                entry = provider_latest.get(key)
                if not entry:
                    print(f"| {label} | _missing_ | Week 2 collection pending |")
                    continue
                value = extract_value(entry)
                detail = entry.get("details")
                if isinstance(detail, dict):
                    detail = json.dumps(detail, ensure_ascii=False)
                elif detail is None:
                    detail = entry.get("timestamp") or entry.get("step") or "Captured from metrics.jsonl"
                print(f"| {label} | {value} | {detail} |")
            print()
        else:
            print("_No ASR or VAD metrics reported (soft gate)._\n")

    if not has_tracked_values:
        print("_ASR/VAD metrics not yet available (soft gate)._\n")
    if printed_latest:
        print("_Threshold gating to be introduced in future sprints._\n")

print("### Vision Keyframe Rates")
print()
if not vision_rates:
    print("_No vision keyframe metrics available._\n")
else:
    print("| Clip | Rate | Timestamp |")
    print("| --- | --- | --- |")
    for clip, entry in sorted(vision_rates.items()):
        value = entry.get("value")
        timestamp = entry.get("timestamp", "n/a")
        print(f"| {clip} | {value} | {timestamp} |")
    print()

print("### Synthetic OCR Precision")
print()
if not ocr_precision:
    print("_No synthetic OCR metrics available._\n")
else:
    value = ocr_precision.get("value")
    timestamp = ocr_precision.get("timestamp", "n/a")
    print("| Metric | Value | Timestamp |")
    print("| --- | --- | --- |")
    print(f"| ocr.precision_synth | {value} | {timestamp} |")
    print()

warnings = []
stderr_messages: list[str] = []
hero_warn_path = Path("artifacts/e2e_hero1_warnings.json")
if hero_warn_path.exists():
    try:
        hero_warnings = json.loads(hero_warn_path.read_text())
        if isinstance(hero_warnings, list):
            hero_lines = [str(item) for item in hero_warnings]
            warnings.extend(hero_lines)
            stderr_messages.extend(
                line.lstrip(":warning:").strip()
                for line in hero_lines
                if line
            )
    except json.JSONDecodeError:
        message = ":warning: Unable to parse hero benchmark warnings artifact."
        warnings.append(message)
        stderr_messages.append(message.lstrip(":warning:").strip())
static_entry = vision_rates.get("static")
if static_entry:
    static_rate = _coerce_float(static_entry.get("value"))
    if static_rate is not None and static_rate > 0.05:
        message = (
            "Static clip keyframe rate {:.3f} exceeds expected maximum (0.05).".format(
                static_rate
            )
        )
        warnings.append(f":warning: {message}")
        stderr_messages.append(message)

if ocr_precision:
    precision_value = _coerce_float(ocr_precision.get("value"))
    if precision_value is not None and precision_value < 0.9:
        message = (
            "Synthetic OCR precision {:.3f} fell below expected minimum (0.9).".format(
                precision_value
            )
        )
        warnings.append(f":warning: {message}")
        stderr_messages.append(message)

if reversal_warning_message:
    warnings.append(reversal_warning_message)
    stderr_messages.append(reversal_warning_message.lstrip(":warning:").strip())

if warnings:
    print("#### Warnings")
    print()
    for line in warnings:
        print(f"- {line}")
    print()

    for line in stderr_messages:
        if not line:
            continue
        print(f"::warning::{line}", file=sys.stderr)
PY
            else
              echo "_ASR/VAD metrics not yet available (soft gate)._";
            fi
          } >> "$GITHUB_STEP_SUMMARY"
          if [ -f "$reversal_flag_file" ]; then
            rate=$(cat "$reversal_flag_file")
            ref_display="${GITHUB_REF_NAME:-${GITHUB_HEAD_REF:-unknown}}"
            echo "::warning:: Audio reversal rate ${rate} exceeded 0.5 threshold on ref ${ref_display}."
          fi

      - name: Upload artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: ci-artifacts
          path: artifacts/
          if-no-files-found: warn
